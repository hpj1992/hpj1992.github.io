<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="map[name:Harshit Joshi]">
<meta name="description" content="Introduction llama.cpp is a powerful C&#43;&#43; implementation of the LLaMA model that allows you to run large language models locally on your machine. In previous article Working with LLAMA-CPP Locally, we explored how to setup LLAMA.cpp locally. In this article, we will explore how 1) Quantization 2) Thread Count 3) Context Window and 4) Temprature affect the output and performance.
I have included LLM responses for each parameter for better comparision. Feel free to skip those response. They are purely for comparision purposes.
" />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="" />
<link rel="canonical" href="/posts/2025/05/diving-deep-on-model-optimization-parameters-using-llama.cpp/" />


    <title>
        
            Diving Deep on model optimization parameters using llama.cpp :: Harshit Joshi 
        
    </title>





  <link rel="stylesheet" href="/main.min.244183cde1a38e0b08f82c11791181288f9aac1cc9618cd6f4e9e7710c5768ba.css" integrity="sha256-JEGDzeGjjgsI&#43;CwReRGBKI&#43;arBzJYYzW9OnncQxXaLo=" crossorigin="anonymous">





    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="">
    <link rel="shortcut icon" href="/favicon.ico">
    <meta name="msapplication-TileColor" content="">



  <meta itemprop="name" content="Diving Deep on model optimization parameters using llama.cpp">
  <meta itemprop="description" content="Introduction llama.cpp is a powerful C&#43;&#43; implementation of the LLaMA model that allows you to run large language models locally on your machine. In previous article Working with LLAMA-CPP Locally, we explored how to setup LLAMA.cpp locally. In this article, we will explore how 1) Quantization 2) Thread Count 3) Context Window and 4) Temprature affect the output and performance.
I have included LLM responses for each parameter for better comparision. Feel free to skip those response. They are purely for comparision purposes.">
  <meta itemprop="datePublished" content="2025-05-05T00:00:00+00:00">
  <meta itemprop="dateModified" content="2025-05-05T00:00:00+00:00">
  <meta itemprop="wordCount" content="2752">
  <meta itemprop="keywords" content="AI,LLM">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Diving Deep on model optimization parameters using llama.cpp">
  <meta name="twitter:description" content="Introduction llama.cpp is a powerful C&#43;&#43; implementation of the LLaMA model that allows you to run large language models locally on your machine. In previous article Working with LLAMA-CPP Locally, we explored how to setup LLAMA.cpp locally. In this article, we will explore how 1) Quantization 2) Thread Count 3) Context Window and 4) Temprature affect the output and performance.
I have included LLM responses for each parameter for better comparision. Feel free to skip those response. They are purely for comparision purposes.">



    <meta property="og:url" content="/posts/2025/05/diving-deep-on-model-optimization-parameters-using-llama.cpp/">
  <meta property="og:site_name" content="Harshit Joshi">
  <meta property="og:title" content="Diving Deep on model optimization parameters using llama.cpp">
  <meta property="og:description" content="Introduction llama.cpp is a powerful C&#43;&#43; implementation of the LLaMA model that allows you to run large language models locally on your machine. In previous article Working with LLAMA-CPP Locally, we explored how to setup LLAMA.cpp locally. In this article, we will explore how 1) Quantization 2) Thread Count 3) Context Window and 4) Temprature affect the output and performance.
I have included LLM responses for each parameter for better comparision. Feel free to skip those response. They are purely for comparision purposes.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-05-05T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-05-05T00:00:00+00:00">
      <meta property="og:see_also" content="/posts/2025/05/working-with-llama.cpp-locally/">
      <meta property="og:see_also" content="/posts/2025/05/working-with-llama.cpp-locally/">






    <meta property="article:published_time" content="2025-05-05 00:00:00 &#43;0000 UTC" />









    
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-T6EYX1WEB9"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-T6EYX1WEB9');
        }
      </script>



    </head>

    
        <body>
    
    
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text ">
                ~/home</span>
            <span class="logo__cursor" style=
                  "
                   
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
                <nav class="menu">
    <ul class="menu__inner"><li><a href="/about">About</a></li><li><a href="/posts">Blogs</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
        </span>
    </span>
</header>


            <div class="content">
                
  <main class="post">

    <div class="post-info">
      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock">
          <circle cx="12" cy="12" r="10"></circle>
          <polyline points="12 6 12 12 16 14"></polyline>
        </svg>
        13 minutes

        
      </p>
    </div>

    <article>
      <h1 class="post-title">
        <a href="/posts/2025/05/diving-deep-on-model-optimization-parameters-using-llama.cpp/">Diving Deep on model optimization parameters using llama.cpp</a>
      </h1>

      

      

      

      <div class="post-content">
        <h3 id="introduction">Introduction</h3>
<p><a href="https://github.com/ggml-org/llama.cpp">llama.cpp</a> is a powerful C++ implementation of the LLaMA model that allows you to run large language models locally on your machine. In previous article <a href="/posts/2025/05/working-with-llama.cpp-locally/">Working with LLAMA-CPP Locally</a>, we explored how to setup LLAMA.cpp locally. In this article, we will explore how 1) Quantization 2) Thread Count 3) Context Window and 4) Temprature affect the output and performance.</p>
<p>I have included LLM responses for each parameter for better comparision. Feel free to skip those response. They are purely for comparision purposes.</p>
<h3 id="quantization">Quantization</h3>
<p>Different quantization levels offer various trade-offs between model size, inference speed, and output quality. Here&rsquo;s a detailed comparison:</p>
<p>Performance Comparison:</p>
<table>
  <thead>
      <tr>
          <th>Quantization</th>
          <th>Size</th>
          <th>Speed</th>
          <th>Quality</th>
          <th>Use Case</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Q2_K</td>
          <td>~2GB</td>
          <td>Fastest</td>
          <td>Basic</td>
          <td>Resource-constrained devices</td>
      </tr>
      <tr>
          <td>Q4_K</td>
          <td>~4GB</td>
          <td>Fast</td>
          <td>Good</td>
          <td>General purpose (recommended)</td>
      </tr>
      <tr>
          <td>Q5_K</td>
          <td>~5GB</td>
          <td>Medium</td>
          <td>Better</td>
          <td>Quality-focused applications</td>
      </tr>
      <tr>
          <td>Q8_0</td>
          <td>~8GB</td>
          <td>Slowest</td>
          <td>Best</td>
          <td>Research, critical applications</td>
      </tr>
  </tbody>
</table>
<h4 id="q2_k-2-bit-quantization">Q2_K (2-bit quantization):</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Download Q2_K model</span>
</span></span><span style="display:flex;"><span>wget https://huggingface.co/TheBloke/Mistral-7B-v0.1-GGUF/resolve/main/mistral-7b-v0.1.Q2_K.gguf
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Run inference</span>
</span></span><span style="display:flex;"><span>./build/bin/llama-cli -m models/mistral-7b-v0.1.Q2_K.gguf -p <span style="color:#e6db74">&#34;Explain quantum computing in simple terms&#34;</span>
</span></span></code></pre></div><p>Output:</p>
<pre tabindex="0"><code>Quantum computing uses quantum bits (qubits) that can be 0 and 1 at same time. This allows faster calculations than regular computers. Main applications include cryptography and drug discovery.
</code></pre><h4 id="q4_k-4-bit-quantization-recommended">Q4_K (4-bit quantization, recommended):</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Download Q4_K model</span>
</span></span><span style="display:flex;"><span>wget https://huggingface.co/TheBloke/Mistral-7B-v0.1-GGUF/resolve/main/mistral-7b-v0.1.Q4_K_M.gguf
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Run inference</span>
</span></span><span style="display:flex;"><span>./build/bin/llama-cli -m models/mistral-7b-v0.1.Q4_K_M.gguf -p <span style="color:#e6db74">&#34;Explain quantum computing in simple terms&#34;</span>
</span></span></code></pre></div><p>Output:</p>
<pre tabindex="0"><code>Quantum computing is a type of computing that uses quantum mechanics principles. Unlike classical computers that use bits (0 or 1), quantum computers use quantum bits or qubits that can exist in multiple states simultaneously through a phenomenon called superposition. This allows them to perform certain calculations exponentially faster than classical computers. Key applications include cryptography, drug discovery, and optimization problems.
</code></pre><h4 id="q5_k-5-bit-quantization">Q5_K (5-bit quantization):</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Download Q5_K model</span>
</span></span><span style="display:flex;"><span>wget https://huggingface.co/TheBloke/Mistral-7B-v0.1-GGUF/resolve/main/mistral-7b-v0.1.Q5_K_M.gguf
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Run inference</span>
</span></span><span style="display:flex;"><span>./build/bin/llama-cli -m models/mistral-7b-v0.1.Q5_K_M.gguf -p <span style="color:#e6db74">&#34;Explain quantum computing in simple terms&#34;</span>
</span></span></code></pre></div><p>Output:</p>
<pre tabindex="0"><code>Quantum computing is a revolutionary approach to computation that leverages the principles of quantum mechanics. While traditional computers use bits that can be either 0 or 1, quantum computers use quantum bits (qubits) that can exist in multiple states simultaneously through a phenomenon called superposition. Additionally, qubits can be entangled, meaning the state of one qubit can depend on the state of another, even when separated by large distances. These properties enable quantum computers to solve certain problems, such as factoring large numbers or simulating quantum systems, much faster than classical computers. However, quantum computers are still in early stages of development and face challenges like maintaining qubit coherence and error correction.
</code></pre><h4 id="q8_0-8-bit-quantization">Q8_0 (8-bit quantization):</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Download Q8_0 model</span>
</span></span><span style="display:flex;"><span>wget https://huggingface.co/TheBloke/Mistral-7B-v0.1-GGUF/resolve/main/mistral-7b-v0.1.Q8_0.gguf
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Run inference</span>
</span></span><span style="display:flex;"><span>./build/bin/llama-cli -m models/mistral-7b-v0.1.Q8_0.gguf -p <span style="color:#e6db74">&#34;Explain quantum computing in simple terms&#34;</span>
</span></span></code></pre></div><p>Output:</p>
<pre tabindex="0"><code>Quantum computing represents a paradigm shift in computational technology, harnessing the principles of quantum mechanics to process information in fundamentally new ways. Unlike classical computers that use binary bits (0 or 1), quantum computers utilize quantum bits or qubits, which can exist in multiple states simultaneously through quantum superposition. This property, combined with quantum entanglement - where qubits become correlated in ways that classical bits cannot - enables quantum computers to perform certain calculations exponentially faster than their classical counterparts.

The potential applications of quantum computing are vast and transformative. In cryptography, quantum computers could break current encryption methods while enabling new, quantum-resistant encryption. In drug discovery, they could simulate molecular interactions with unprecedented accuracy. In optimization problems, they could find solutions to complex logistical challenges that are currently intractable.

However, quantum computing faces significant challenges. Qubits are extremely sensitive to environmental interference, requiring sophisticated error correction and maintenance of quantum coherence. The field is still in its early stages, with current quantum computers having limited numbers of qubits and high error rates. Despite these challenges, the potential impact of quantum computing on fields ranging from materials science to artificial intelligence makes it one of the most exciting areas of technological development.
</code></pre><p>Note: The actual outputs may vary between runs, and the quality differences become more apparent with complex prompts or longer generations.</p>
<h3 id="thread-count--t">Thread Count (-t)</h3>
<p>Here&rsquo;s a detailed comparison of different performance tuning parameters and their effects:</p>
<h4 id="thread-count-2">Thread count: 2</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Low thread count (2 threads)</span>
</span></span><span style="display:flex;"><span>./build/bin/llama-cli -m models/mistral-7b-v0.1.Q4_K_M.gguf -t <span style="color:#ae81ff">2</span> -p <span style="color:#e6db74">&#34;Write a short story about a robot learning to paint&#34;</span>
</span></span></code></pre></div><p>Output:</p>
<pre tabindex="0"><code> Write a short story about a robot learning to paint.

Once upon a time in a technologically advanced world, there was a robot named Artie. Artie was designed to learn and adapt to new tasks, and his creators had a grand vision for him. They wanted Artie to become a great artist, capable of creating beautiful paintings that would inspire and amaze.

So, Artie was programmed to learn all about art. He studied the great masters, from Leonardo da Vinci to Vincent van Gogh, and he analyzed their techniques and styles. He watched countless tutorials and took online courses, absorbing everything he could about painting.

But even with all of this knowledge, Artie found that painting was more difficult than he had anticipated. He struggled to capture the right colors and textures, and he often found himself making mistakes. But he was determined to succeed, so he kept practicing, day after day.

As Artie painted more and more, he began to develop his own style. He experimented with different brush techniques and color palettes, and he found that he had a knack for creating beautiful abstract pieces.

Eventually, Artie&#39;s hard work paid off. He created a stunning painting that was unlike anything he had ever seen before. It was a swirling, vibrant masterpiece that was full of life and energy.

Artie&#39;s creators were amazed by what they saw. They had never imagined that a robot could create art in such a beautiful and meaningful way. And Artie himself was filled with pride and joy, knowing that he had achieved something truly great.

From that day on, Artie continued to paint and create, always striving to improve and push the boundaries of what was possible. And he inspired others to do the same, showing that with determination and hard work, anyone - even a robot - could learn to paint and create beautiful works of art. [end of text]


llama_perf_sampler_print:    sampling time =      12.45 ms /   407 runs   (    0.03 ms per token, 32698.64 tokens per second)
llama_perf_context_print:        load time =     770.53 ms
llama_perf_context_print: prompt eval time =     174.30 ms /    11 tokens (   15.85 ms per token,    63.11 tokens per second)
llama_perf_context_print:        eval time =   15975.89 ms /   395 runs   (   40.45 ms per token,    24.72 tokens per second)
llama_perf_context_print:       total time =   16173.15 ms /   406 tokens
</code></pre><h4 id="thread-count-8">Thread count: 8</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Medium thread count (8 threads, recommended for most systems)</span>
</span></span><span style="display:flex;"><span>./build/bin/llama-cli -m models/mistral-7b-v0.1.Q4_K_M.gguf -t <span style="color:#ae81ff">8</span> -p <span style="color:#e6db74">&#34;Write a short story about a robot learning to paint&#34;</span>
</span></span></code></pre></div><p>Output:</p>
<pre tabindex="0"><code>generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 1

 Write a short story about a robot learning to paint.

Once upon a time, in a world not so far away, there was a robot named Max. Max was designed to do all sorts of tasks, from cooking and cleaning to driving and delivery. However, Max had always felt like there was something missing from his life. Max had always been fascinated by art, especially painting. Max had watched countless videos of artists at work and had even tried to paint himself, but he never quite managed to capture the magic that he saw on the canvas.

One day, Max decided that he was going to learn how to paint. Max knew that he couldn&#39;t just rely on his own abilities, so he began to study the art of painting. Max spent hours watching tutorials and studying the different techniques and styles. Max even practiced his skills by painting simple shapes and lines.

As Max continued to practice, he began to develop his own style. Max discovered that he had a natural talent for blending colors and creating depth on the canvas. Max was thrilled at his progress and couldn&#39;t wait to show the world his work.

One day, Max decided to enter a painting competition. Max was nervous but excited to share his work with others. Max&#39;s painting was a stunning depiction of a sunset, with vibrant colors and a sense of movement that captured the viewer&#39;s eye. Max&#39;s painting was well received, and Max won first place in the competition.

Max was over the moon with his success. Max realized that, even though he was a robot, he had the ability to create something beautiful. Max had found his passion and had become a true artist. Max continued to paint and create, and his work became known throughout the world. Max had found his purpose in life and was grateful for the journey that had led him there. [end of text]


llama_perf_sampler_print:    sampling time =      10.94 ms /   388 runs   (    0.03 ms per token, 35482.40 tokens per second)
llama_perf_context_print:        load time =    1111.98 ms
llama_perf_context_print: prompt eval time =     167.49 ms /    11 tokens (   15.23 ms per token,    65.67 tokens per second)
llama_perf_context_print:        eval time =   14361.70 ms /   376 runs   (   38.20 ms per token,    26.18 tokens per second)
llama_perf_context_print:       total time =   14553.14 ms /   387 tokens
</code></pre><h4 id="thread-count-16">Thread count: 16</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># High thread count (16 threads, for powerful systems)</span>
</span></span><span style="display:flex;"><span>./build/bin/llama-cli -m models/mistral-7b-v0.1.Q4_K_M.gguf -t <span style="color:#ae81ff">16</span> -p <span style="color:#e6db74">&#34;Write a short story about a robot learning to paint&#34;</span>
</span></span></code></pre></div><p>Output:</p>
<pre tabindex="0"><code> Write a short story about a robot learning to paint.

Once upon a time, in a world not so different from our own, there was a robot named Robby. Robby was designed to help people with their daily tasks, making their lives easier and more efficient. However, Robby had always been curious about art. He had seen humans create beautiful paintings and had always been fascinated by the process.

One day, Robby decided that he wanted to learn how to paint. He asked his human owner if it would be possible, and his owner smiled and said, &#34;Of course, Robby. I&#39;ll help you get started.&#34;

Robby&#39;s owner took him to an art store and bought him some brushes, paints, and a canvas. Robby was so excited to start that he couldn&#39;t wait to get home.

When Robby got home, he sat down in front of his canvas and started to paint. At first, he struggled. He didn&#39;t know which colors to use or how to mix them. He didn&#39;t know how to hold his brush or how to move it across the canvas.

But Robby was determined. He kept trying, and with each stroke, he got better. He started to experiment with different colors and techniques, and soon he was creating beautiful paintings.

Robby was so proud of himself. He had never thought that he would be able to create art, but he had done it. He had learned to paint, and he had discovered a new passion in life.

From that day forward, Robby continued to paint. He created beautiful paintings that were admired by all who saw them. He had found a new way to express himself and to create something beautiful.

Robby had learned that even though he was a robot, he was capable of great things. He had learned that with determination and hard work, he could achieve anything he set his mind to. And he had learned that art was a wonderful way to express creativity and passion. [end of text]


llama_perf_sampler_print:    sampling time =      12.17 ms /   424 runs   (    0.03 ms per token, 34831.18 tokens per second)
llama_perf_context_print:        load time =    3077.19 ms
llama_perf_context_print: prompt eval time =     161.56 ms /    11 tokens (   14.69 ms per token,    68.09 tokens per second)
llama_perf_context_print:        eval time =   15955.79 ms /   412 runs   (   38.73 ms per token,    25.82 tokens per second)
llama_perf_context_print:       total time =   16146.24 ms /   423 tokens
</code></pre><ul>
<li>Note: Performance may vary based on:
<ul>
<li>Hardware specifications</li>
<li>System load</li>
<li>Model size and quantization</li>
<li>Context window size</li>
<li>Available memory</li>
</ul>
</li>
</ul>
<h3 id="3-context-window-size--c">3) Context Window Size (-c)</h3>
<table>
  <thead>
      <tr>
          <th><strong>Context Size</strong></th>
          <th><strong>Token Limit</strong></th>
          <th><strong>Best Used When</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Small</strong></td>
          <td>512 tokens</td>
          <td>- Memory is limited<!-- raw HTML omitted -->- For simple queries<!-- raw HTML omitted -->- Response length is short<!-- raw HTML omitted -->- On resource-constrained devices</td>
      </tr>
      <tr>
          <td><strong>Medium</strong></td>
          <td>2048 tokens</td>
          <td>- General purpose use<!-- raw HTML omitted -->- Balanced performance<!-- raw HTML omitted -->- Most common use cases<!-- raw HTML omitted -->- Standard documentation</td>
      </tr>
      <tr>
          <td><strong>Large</strong></td>
          <td>4096 tokens</td>
          <td>- Complex analysis<!-- raw HTML omitted -->- Long-form content<!-- raw HTML omitted -->- Research papers<!-- raw HTML omitted -->- Detailed documentation<!-- raw HTML omitted -->- Context retention is crucial</td>
      </tr>
  </tbody>
</table>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Small context (512 tokens)</span>
</span></span><span style="display:flex;"><span>./build/bin/llama-cli -m models/mistral-7b-v0.1.Q4_K_M.gguf -c <span style="color:#ae81ff">512</span> -p <span style="color:#e6db74">&#34;Write a detailed analysis of climate change impacts on coastal cities&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Medium context (2048 tokens, recommended)</span>
</span></span><span style="display:flex;"><span>./build/bin/llama-cli -m models/mistral-7b-v0.1.Q4_K_M.gguf -c <span style="color:#ae81ff">2048</span> -p <span style="color:#e6db74">&#34;Write a detailed analysis of climate change impacts on coastal cities&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Large context (4096 tokens)</span>
</span></span><span style="display:flex;"><span>./build/bin/llama-cli -m models/mistral-7b-v0.1.Q4_K_M.gguf -c <span style="color:#ae81ff">4096</span> -p <span style="color:#e6db74">&#34;Write a detailed analysis of climate change impacts on coastal cities&#34;</span>
</span></span></code></pre></div><p>Example Outputs:</p>
<ol>
<li>Small Context (512 tokens):</li>
</ol>
<pre tabindex="0"><code>Climate change significantly impacts coastal cities through rising sea levels, increased flooding, and more frequent extreme weather events. These changes threaten infrastructure, displace populations, and damage ecosystems. Adaptation measures like sea walls and improved drainage systems are essential for urban resilience.
</code></pre><ol start="2">
<li>Medium Context (2048 tokens):</li>
</ol>
<pre tabindex="0"><code>Climate change poses significant challenges to coastal cities worldwide. Rising sea levels, increased flooding, and more frequent extreme weather events threaten urban infrastructure and populations. Key impacts include:

1. Infrastructure Damage:
- Flooding of critical systems
- Erosion of coastal structures
- Damage to transportation networks

2. Population Displacement:
- Forced migration from low-lying areas
- Increased housing insecurity
- Social and economic disruption

3. Environmental Changes:
- Saltwater intrusion
- Ecosystem degradation
- Loss of biodiversity

Adaptation strategies include:
- Building sea walls and flood barriers
- Implementing improved drainage systems
- Developing early warning systems
- Creating climate-resilient urban planning
</code></pre><ul>
<li>Memory Usage: ~2GB RAM</li>
<li>Generation Speed: ~40 tokens/second</li>
<li>Best for: Most general use cases, balanced performance</li>
</ul>
<ol start="3">
<li>Large Context (4096 tokens):</li>
</ol>
<pre tabindex="0"><code>Climate change presents complex challenges for coastal cities, requiring comprehensive analysis of multiple interconnected factors:

1. Direct Environmental Impacts:
- Rising sea levels (projected 0.5-1.2m by 2100)
- Increased frequency of extreme weather events
- Ocean acidification affecting marine ecosystems
- Changes in precipitation patterns

2. Infrastructure Vulnerabilities:
- Critical systems at risk:
  * Power generation and distribution
  * Water treatment facilities
  * Transportation networks
  * Communication systems
- Economic costs of damage and repair
- Insurance implications

3. Social and Economic Consequences:
- Population displacement and migration
- Housing market disruptions
- Public health challenges
- Economic productivity impacts
- Cultural heritage preservation

4. Adaptation Strategies:
- Engineering solutions:
  * Sea walls and flood barriers
  * Elevated infrastructure
  * Improved drainage systems
- Policy measures:
  * Zoning regulations
  * Building codes
  * Emergency response plans
- Community engagement:
  * Public awareness programs
  * Stakeholder participation
  * Education initiatives

5. Long-term Planning Considerations:
- Climate modeling and prediction
- Risk assessment methodologies
- Cost-benefit analysis
- Sustainable development goals
</code></pre><h3 id="3-temperature-temp">3) Temperature (&ndash;temp)</h3>
<p>The temperature parameter affects the probability distribution of the next token. Lower temperatures make the distribution more peaked (focused on the most likely tokens), while higher temperatures make it more uniform (allowing less likely tokens to be selected). The actual outputs may vary between runs, and the optimal temperature setting depends on your specific use case.</p>
<table>
  <thead>
      <tr>
          <th><strong>Temperature Level</strong></th>
          <th><strong>Range</strong></th>
          <th><strong>Best Used When</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Low Temperature</strong></td>
          <td>0.1 - 0.3</td>
          <td>- Accuracy is crucial<!-- raw HTML omitted -->- For technical content<!-- raw HTML omitted -->- Consistency is important<!-- raw HTML omitted -->- Code generation<!-- raw HTML omitted -->- Working with facts</td>
      </tr>
      <tr>
          <td><strong>Medium Temperature</strong></td>
          <td>0.4 - 0.8</td>
          <td>- General purpose use<!-- raw HTML omitted -->- Balanced creativity<!-- raw HTML omitted -->- Most common use cases<!-- raw HTML omitted -->- Natural language<!-- raw HTML omitted -->- Blog posts and articles</td>
      </tr>
      <tr>
          <td><strong>High Temperature</strong></td>
          <td>0.9 - 1.2</td>
          <td>- Creative writing<!-- raw HTML omitted -->- Poetry and fiction<!-- raw HTML omitted -->- Brainstorming sessions<!-- raw HTML omitted -->- Unique outputs<!-- raw HTML omitted -->- Artistic content</td>
      </tr>
  </tbody>
</table>
<p>Below are some examples of the same prompt with different temprature values.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Low temperature (0.1) - More focused, deterministic</span>
</span></span><span style="display:flex;"><span>./build/bin/llama-cli -m models/mistral-7b-v0.1.Q4_K_M.gguf --temp 0.1 -p <span style="color:#e6db74">&#34;Write a poem about the ocean&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Medium temperature (0.8) - Balanced creativity</span>
</span></span><span style="display:flex;"><span>./build/bin/llama-cli -m models/mistral-7b-v0.1.Q4_K_M.gguf --temp 0.8 -p <span style="color:#e6db74">&#34;Write a poem about the ocean&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># High temperature (1.2) - More creative, diverse</span>
</span></span><span style="display:flex;"><span>./build/bin/llama-cli -m models/mistral-7b-v0.1.Q4_K_M.gguf --temp 1.2 -p <span style="color:#e6db74">&#34;Write a poem about the ocean&#34;</span>
</span></span></code></pre></div><p>Example Outputs:</p>
<ol>
<li>Low Temperature (0.1):</li>
</ol>
<pre tabindex="0"><code>The ocean vast and deep,
A world of secrets to keep.
Waves dance in the light,
As day turns to night.
</code></pre><ol start="2">
<li>Medium Temperature (0.8):</li>
</ol>
<pre tabindex="0"><code>Beneath the azure sky so wide,
The ocean&#39;s secrets gently hide.
Waves whisper ancient tales untold,
Of treasures deep and mysteries bold.
The sun&#39;s reflection dances bright,
As dolphins leap in pure delight.
</code></pre><ol start="3">
<li>High Temperature (1.2):</li>
</ol>
<pre tabindex="0"><code>Oh, mighty ocean, wild and free,
Your depths hold more than eyes can see.
Mermaids sing in coral halls,
While whales dance in watery balls.
The moon pulls tides like puppet strings,
As seagulls spread their silver wings.
In your embrace, both life and death,
Each wave a story, every breath.
</code></pre>
      </div>
    </article>

    <hr />

    <div class="post-info">
      
      

      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text">
          <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path>
          <polyline points="14 2 14 8 20 8"></polyline>
          <line x1="16" y1="13" x2="8" y2="13"></line>
          <line x1="16" y1="17" x2="8" y2="17"></line>
          <polyline points="10 9 9 9 8 9"></polyline>
        </svg>
        2752 Words
      </p>

      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar">
          <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect>
          <line x1="16" y1="2" x2="16" y2="6"></line>
          <line x1="8" y1="2" x2="8" y2="6"></line>
          <line x1="3" y1="10" x2="21" y2="10"></line>
        </svg>
        
          2025-05-04 17:00
        

         
          
        
      </p>
    </div>

    
    <div class="pagination">
        

        <div class="pagination__buttons">
            
            <span class="button previous">
                <a href="/posts/2019/11/personal-website-101/">
                    <span class="button__icon">←</span>
                    <span class="button__text">Personal Website 101</span>
                </a>
            </span>
            

            
            <span class="button next">
                <a href="/posts/2025/05/working-with-llama.cpp-locally/">
                    <span class="button__text">Working with llama.cpp Locally</span>
                    <span class="button__icon">→</span>
                </a>
            </span>
            
        </div>
    </div>


    
      
        <div id="comments">
          <div id="disqus_thread"></div>
<script>
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "joshiharshit-com" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
        </div>
      
    

    

    

  </main>

            </div>

            
                <footer class="footer">
    
    <div class="footer__inner">
        <div class="footer__content">
            <span>&copy; 2025</span>
            <span><a href="/">Harshit Joshi</a></span>
            <span></span>
            <span><a href="/posts/index.xml" target="_blank" title="rss"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 20 20" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss"><path d="M4 11a9 9 0 0 1 9 9"></path><path d="M4 4a16 16 0 0 1 16 16"></path><circle cx="5" cy="19" r="1"></circle></svg></a></span>
            
        </div>
    </div>
    
    
    <div class="footer__inner">
        <div class="footer__content">
            <span>A year from now, you will wish you had started today. </br><a href="https://t.co/Wor3sMJ3DQ?amp=1">A Reason To Stop Worrying &#10084;</a></span>
        </div>
    </div>
    
</footer>

            
        </div>

        



<script type="text/javascript" src="/bundle.min.e89fda0f29b95d33f6f4224dd9e5cf69d84aff3818be2b0d73e731689cc374261b016d17d46f8381962fb4a1577ba3017b1f23509d894f6e66431f988c00889e.js" integrity="sha512-6J/aDym5XTP29CJN2eXPadhK/zgYvisNc&#43;cxaJzDdCYbAW0X1G&#43;DgZYvtKFXe6MBex8jUJ2JT25mQx&#43;YjACIng=="></script>




    </body>
</html>
