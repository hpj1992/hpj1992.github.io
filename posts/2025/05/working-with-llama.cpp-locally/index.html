<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="map[name:Harshit Joshi]">
<meta name="description" content="Introduction llama.cpp is a powerful C&#43;&#43; implementation of the LLaMA model that allows you to run large language models locally on your machine. This guide will walk you through the process of setting up and using llama.cpp effectively. Official github repo has enough details, however I have still more granular steps here and sample outputs.
Prerequisites C&#43;&#43; compiler (GCC or Clang) CMake Git Sufficient disk space for model weights Adequate RAM (8GB minimum recommended) Installation Steps Clone the repository: git clone https://github.com/ggerganov/llama.cpp.git cd llama.cpp Build the project: mkdir build cd build cmake .. cmake --build . --config Release Downloading Models You can download various LLaMA models from Hugging Face. Here are multiple methods:
" />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="" />
<link rel="canonical" href="/posts/2025/05/working-with-llama.cpp-locally/" />


    <title>
        
            Working with llama.cpp Locally :: Harshit Joshi 
        
    </title>





  <link rel="stylesheet" href="/main.min.244183cde1a38e0b08f82c11791181288f9aac1cc9618cd6f4e9e7710c5768ba.css" integrity="sha256-JEGDzeGjjgsI&#43;CwReRGBKI&#43;arBzJYYzW9OnncQxXaLo=" crossorigin="anonymous">





    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="">
    <link rel="shortcut icon" href="/favicon.ico">
    <meta name="msapplication-TileColor" content="">



  <meta itemprop="name" content="Working with llama.cpp Locally">
  <meta itemprop="description" content="Introduction llama.cpp is a powerful C&#43;&#43; implementation of the LLaMA model that allows you to run large language models locally on your machine. This guide will walk you through the process of setting up and using llama.cpp effectively. Official github repo has enough details, however I have still more granular steps here and sample outputs.
Prerequisites C&#43;&#43; compiler (GCC or Clang) CMake Git Sufficient disk space for model weights Adequate RAM (8GB minimum recommended) Installation Steps Clone the repository: git clone https://github.com/ggerganov/llama.cpp.git cd llama.cpp Build the project: mkdir build cd build cmake .. cmake --build . --config Release Downloading Models You can download various LLaMA models from Hugging Face. Here are multiple methods:">
  <meta itemprop="datePublished" content="2025-05-05T00:00:00+00:00">
  <meta itemprop="dateModified" content="2025-05-05T00:00:00+00:00">
  <meta itemprop="wordCount" content="637">
  <meta itemprop="keywords" content="AI,LLM">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Working with llama.cpp Locally">
  <meta name="twitter:description" content="Introduction llama.cpp is a powerful C&#43;&#43; implementation of the LLaMA model that allows you to run large language models locally on your machine. This guide will walk you through the process of setting up and using llama.cpp effectively. Official github repo has enough details, however I have still more granular steps here and sample outputs.
Prerequisites C&#43;&#43; compiler (GCC or Clang) CMake Git Sufficient disk space for model weights Adequate RAM (8GB minimum recommended) Installation Steps Clone the repository: git clone https://github.com/ggerganov/llama.cpp.git cd llama.cpp Build the project: mkdir build cd build cmake .. cmake --build . --config Release Downloading Models You can download various LLaMA models from Hugging Face. Here are multiple methods:">



    <meta property="og:url" content="/posts/2025/05/working-with-llama.cpp-locally/">
  <meta property="og:site_name" content="Harshit Joshi">
  <meta property="og:title" content="Working with llama.cpp Locally">
  <meta property="og:description" content="Introduction llama.cpp is a powerful C&#43;&#43; implementation of the LLaMA model that allows you to run large language models locally on your machine. This guide will walk you through the process of setting up and using llama.cpp effectively. Official github repo has enough details, however I have still more granular steps here and sample outputs.
Prerequisites C&#43;&#43; compiler (GCC or Clang) CMake Git Sufficient disk space for model weights Adequate RAM (8GB minimum recommended) Installation Steps Clone the repository: git clone https://github.com/ggerganov/llama.cpp.git cd llama.cpp Build the project: mkdir build cd build cmake .. cmake --build . --config Release Downloading Models You can download various LLaMA models from Hugging Face. Here are multiple methods:">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-05-05T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-05-05T00:00:00+00:00">
      <meta property="og:see_also" content="/posts/2025/05/diving-deep-on-model-optimization-parameters-using-llama.cpp/">
      <meta property="og:see_also" content="/posts/2025/05/diving-deep-on-model-optimization-parameters-using-llama.cpp/">






    <meta property="article:published_time" content="2025-05-05 00:00:00 &#43;0000 UTC" />









    
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-T6EYX1WEB9"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-T6EYX1WEB9');
        }
      </script>



    </head>

    
        <body>
    
    
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text ">
                ~/home</span>
            <span class="logo__cursor" style=
                  "
                   
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
                <nav class="menu">
    <ul class="menu__inner"><li><a href="/about">About</a></li><li><a href="/posts">Blogs</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
        </span>
    </span>
</header>


            <div class="content">
                
  <main class="post">

    <div class="post-info">
      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock">
          <circle cx="12" cy="12" r="10"></circle>
          <polyline points="12 6 12 12 16 14"></polyline>
        </svg>
        3 minutes

        
      </p>
    </div>

    <article>
      <h1 class="post-title">
        <a href="/posts/2025/05/working-with-llama.cpp-locally/">Working with llama.cpp Locally</a>
      </h1>

      

      

      

      <div class="post-content">
        <h3 id="introduction">Introduction</h3>
<p><a href="https://github.com/ggml-org/llama.cpp">llama.cpp</a> is a powerful C++ implementation of the LLaMA model that allows you to run large language models locally on your machine. This guide will walk you through the process of setting up and using llama.cpp effectively. Official github repo has enough details, however I have still more granular steps here and sample outputs.</p>
<h3 id="prerequisites">Prerequisites</h3>
<ul>
<li>C++ compiler (GCC or Clang)</li>
<li>CMake</li>
<li>Git</li>
<li>Sufficient disk space for model weights</li>
<li>Adequate RAM (8GB minimum recommended)</li>
</ul>
<h3 id="installation-steps">Installation Steps</h3>
<ol>
<li>Clone the repository:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>git clone https://github.com/ggerganov/llama.cpp.git
</span></span><span style="display:flex;"><span>cd llama.cpp
</span></span></code></pre></div><ol start="2">
<li>Build the project:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>mkdir build
</span></span><span style="display:flex;"><span>cd build
</span></span><span style="display:flex;"><span>cmake ..
</span></span><span style="display:flex;"><span>cmake --build . --config Release
</span></span></code></pre></div><h3 id="downloading-models">Downloading Models</h3>
<p>You can download various LLaMA models from Hugging Face. Here are multiple methods:</p>
<ol>
<li>Using wget (recommended for simplicity):</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Create a models directory</span>
</span></span><span style="display:flex;"><span>mkdir -p models
</span></span><span style="display:flex;"><span>cd models
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Download Mistral-7B</span>
</span></span><span style="display:flex;"><span>wget https://huggingface.co/TheBloke/Mistral-7B-v0.1-GGUF/resolve/main/mistral-7b-v0.1.Q4_K_M.gguf
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Download Llama-2-7B</span>
</span></span><span style="display:flex;"><span>wget https://huggingface.co/TheBloke/Llama-2-7B-GGUF/resolve/main/llama2-7b.Q4_K_M.gguf
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Download Phi-2</span>
</span></span><span style="display:flex;"><span>wget https://huggingface.co/TheBloke/phi-2-GGUF/resolve/main/phi-2.Q4_K_M.gguf
</span></span></code></pre></div><ol start="2">
<li>Using curl:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Create a models directory</span>
</span></span><span style="display:flex;"><span>mkdir -p models
</span></span><span style="display:flex;"><span>cd models
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Download Mistral-7B</span>
</span></span><span style="display:flex;"><span>curl -L https://huggingface.co/TheBloke/Mistral-7B-v0.1-GGUF/resolve/main/mistral-7b-v0.1.Q4_K_M.gguf -o mistral-7b-v0.1.Q4_K_M.gguf
</span></span></code></pre></div><ol start="3">
<li>Using Python (if you prefer):</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>pip install huggingface_hub
</span></span><span style="display:flex;"><span>huggingface-cli download TheBloke/Mistral-7B-v0.1-GGUF mistral-7b-v0.1.Q4_K_M.gguf --local-dir ./models
</span></span></code></pre></div><p>Popular model options and their sizes:</p>
<ul>
<li>Llama-2-7b (4-bit quantized): ~4GB</li>
<li>Llama-2-13b (4-bit quantized): ~7GB</li>
<li>Mistral-7B (4-bit quantized): ~4GB</li>
<li>Phi-2 (4-bit quantized): ~2GB</li>
</ul>
<p>Note: The <code>Q4_K_M</code> suffix indicates 4-bit quantization, which provides a good balance between model size and performance. Other quantization levels are available (Q2_K, Q5_K, Q8_0) depending on your needs for quality vs. size.</p>
<h3 id="running-inference">Running Inference</h3>
<h4 id="mode-1-single-shot-with-inline-prompt">Mode 1: Single shot with inline prompt</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>cd /Users/hpjoshi/Documents/AppliedAI/llama.cpp <span style="color:#f92672">&amp;&amp;</span> ./build/bin/llama-cli -m /Users/hpjoshi/Documents/AppliedAI/llm/models/mistral-q4.gguf -p <span style="color:#e6db74">&#34;write a simple HTML and CSS code to create a bouncing ball animation using jQuery&#34;</span>
</span></span></code></pre></div><p>The command structure is:</p>
<ul>
<li><code>./build/bin/llama-cli</code>: The compiled executable</li>
<li><code>-m</code>: Path to your model file (typically a .gguf file)</li>
<li><code>-p</code>: Your prompt or input text</li>
</ul>
<p>Common model paths:</p>
<ul>
<li>Local models: <code>../llm/models/your-model.gguf</code></li>
<li>Absolute path: <code>/path/to/your/model.gguf</code></li>
</ul>
<p>You can also add additional parameters:</p>
<ul>
<li><code>-n</code>: Number of tokens to generate</li>
<li><code>-t</code>: Number of threads to use</li>
<li><code>-c</code>: Context window size</li>
</ul>
<h3 id="sample-outputs">Sample Outputs</h3>
<p>Here are some example outputs from different prompts using Mistral-7B:</p>
<ol>
<li>Joke Generation:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>./build/bin/llama-cli -m ../llm/models/mistral-q4.gguf -p <span style="color:#e6db74">&#34;tell me a joke&#34;</span>
</span></span></code></pre></div><p>Output:</p>
<pre tabindex="0"><code>Why don&#39;t scientists trust atoms?
Because they make up everything!
</code></pre><ol start="2">
<li>Code Generation:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>./build/bin/llama-cli -m ../llm/models/mistral-q4.gguf -p <span style="color:#e6db74">&#34;write a python function to calculate fibonacci numbers&#34;</span>
</span></span></code></pre></div><p>Output:</p>
<pre tabindex="0"><code> write a python function to calculate fibonacci numbers using recursion
function fibonacci(n):
    if n &lt;= 1:
        return n
    else:
        return(fibonacci(n-1) + fibonacci(n-2))

# test the function
print(fibonacci(8)) # 21 [end of text]
</code></pre><ol start="3">
<li>Question Answering:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>./build/bin/llama-cli -m ../llm/models/mistral-q4.gguf -p <span style="color:#e6db74">&#34;What is the capital of France?&#34;</span>
</span></span></code></pre></div><p>Output:</p>
<pre tabindex="0"><code>The capital of France is Paris. It is the largest city in France and serves as the country&#39;s political, economic, and cultural center.
</code></pre><p>Note: The actual outputs may vary slightly between runs and different model versions. The quality of responses depends on the model size, quantization level, and the specific prompt used.</p>
<h4 id="mode-2-chat-mode">Mode 2: Chat mode</h4>
<p>With below command, you can run any model locally. This run locally on specifided port using OpenAPI.</p>
<pre tabindex="0"><code>./build/bin/llama-server -m ../llm/models/mistral-q4.gguf
</code></pre><p>This is how sample chat-mode UI looks.</p>
<p><img src="/images/llama-cpp-chat-mode.png" alt="Model Loading"></p>
<h3 id="some-more-examples">Some more examples</h3>
<p>Here are some examples of llama.cpp in action:</p>
<ol>
<li>
<p>Model Loading Process:
<img src="/images/llama-cpp-loading.png" alt="Model Loading">
<em>The model loading process showing Metal GPU initialization and model metadata</em></p>
</li>
<li>
<p>Code Generation Example:
<img src="/images/llama-cpp-code.png" alt="Code Generation">
<em>Example of code generation showing a complete HTML/CSS/JS solution for a bouncing ball animation</em></p>
</li>
<li>
<p>Interactive Response Example:
<img src="/images/llama-cpp-travel.png" alt="Interactive Response">
<em>Example of a detailed, structured response to a travel-related question</em></p>
</li>
</ol>
<p>Note: The actual outputs may vary slightly between runs and different model versions. The quality of responses depends on the model size, quantization level, and the specific prompt used.</p>
<h3 id="conclusion">Conclusion</h3>
<p>llama.cpp provides a powerful way to run large language models locally, giving you full control over your AI applications while maintaining privacy and reducing costs. The flexibility in model selection, quantization options, and performance tuning makes it an excellent choice for both development and production use cases.</p>
<blockquote>
<p>Next in the series: <a href="/posts/2025/05/diving-deep-on-model-optimization-parameters-using-llama.cpp/">Diving Deep into model parameters with llama.cpp</a></p></blockquote>

      </div>
    </article>

    <hr />

    <div class="post-info">
      
      

      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text">
          <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path>
          <polyline points="14 2 14 8 20 8"></polyline>
          <line x1="16" y1="13" x2="8" y2="13"></line>
          <line x1="16" y1="17" x2="8" y2="17"></line>
          <polyline points="10 9 9 9 8 9"></polyline>
        </svg>
        637 Words
      </p>

      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar">
          <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect>
          <line x1="16" y1="2" x2="16" y2="6"></line>
          <line x1="8" y1="2" x2="8" y2="6"></line>
          <line x1="3" y1="10" x2="21" y2="10"></line>
        </svg>
        
          2025-05-04 17:00
        

         
          
        
      </p>
    </div>

    
    <div class="pagination">
        

        <div class="pagination__buttons">
            
            <span class="button previous">
                <a href="/posts/2025/05/diving-deep-on-model-optimization-parameters-using-llama.cpp/">
                    <span class="button__icon">‚Üê</span>
                    <span class="button__text">Diving Deep on model optimization parameters using llama.cpp</span>
                </a>
            </span>
            

            
        </div>
    </div>


    
      
        <div id="comments">
          <div id="disqus_thread"></div>
<script>
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "joshiharshit-com" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
        </div>
      
    

    

    

  </main>

            </div>

            
                <footer class="footer">
    
    <div class="footer__inner">
        <div class="footer__content">
            <span>&copy; 2025</span>
            <span><a href="/">Harshit Joshi</a></span>
            <span></span>
            <span><a href="/posts/index.xml" target="_blank" title="rss"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 20 20" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss"><path d="M4 11a9 9 0 0 1 9 9"></path><path d="M4 4a16 16 0 0 1 16 16"></path><circle cx="5" cy="19" r="1"></circle></svg></a></span>
            
        </div>
    </div>
    
    
    <div class="footer__inner">
        <div class="footer__content">
            <span>A year from now, you will wish you had started today. </br><a href="https://t.co/Wor3sMJ3DQ?amp=1">A Reason To Stop Worrying &#10084;</a></span>
        </div>
    </div>
    
</footer>

            
        </div>

        



<script type="text/javascript" src="/bundle.min.e89fda0f29b95d33f6f4224dd9e5cf69d84aff3818be2b0d73e731689cc374261b016d17d46f8381962fb4a1577ba3017b1f23509d894f6e66431f988c00889e.js" integrity="sha512-6J/aDym5XTP29CJN2eXPadhK/zgYvisNc&#43;cxaJzDdCYbAW0X1G&#43;DgZYvtKFXe6MBex8jUJ2JT25mQx&#43;YjACIng=="></script>




    </body>
</html>
